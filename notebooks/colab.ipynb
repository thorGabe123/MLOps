{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8icSSayr2gA"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"hyperparameters\": {\n",
        "        \"batch_size\": 2,\n",
        "        \"epochs\": 5,\n",
        "        \"learning_rate\": 5e-4,\n",
        "        \"warmup_steps\": 1e2,\n",
        "        \"epsilon\": 1e-08,\n",
        "        \"sample_every\": 200,\n",
        "        \"seed_val\" : 42\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31-Rah3As1Pb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53b1e7a5-857b-4563-a7be-ffbee1f557da"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxBZfwk-uP52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "174c533c-221d-436c-9456-8a4da0574f21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jan 17 14:45:41 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   70C    P0              32W /  70W |   6095MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QSUBm5Ur2gB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "# loading\n",
        "# config = OmegaConf.load(\"project_name/config.yaml\")\n",
        "\n",
        "lr = config[\"hyperparameters\"][\"learning_rate\"]\n",
        "eps = config[\"hyperparameters\"][\"epsilon\"]\n",
        "\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, lr=lr, eps=eps, model_version=\"gpt2\"):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load the GPTModel\n",
        "        configuration = GPT2Config.from_pretrained(model_version, output_hidden_states=False)\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_version, config=configuration)\n",
        "        # Load the GPT tokenizer.\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(\n",
        "            model_version, bos_token=\"<|startoftext|>\", eos_token=\"<|endoftext|>\", pad_token=\"<|pad|>\"\n",
        "        )  # gpt2-medium)\n",
        "        # Setting parameters\n",
        "        self.lr = lr\n",
        "        self.epsilon = eps\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, token_type_ids=None)\n",
        "        return outputs\n",
        "\n",
        "    def resize_token_embeddings(self, size):\n",
        "        return self.model.resize_token_embeddings(size)\n",
        "\n",
        "    def generate(self, inputs=None, bos_token_id=None, max_output_length=200, num_return_sequences=1):\n",
        "        # Use the generate method from the GPTmodel model\n",
        "        generated_text = []\n",
        "        sample_outputs = self.model.generate(\n",
        "            inputs=inputs,\n",
        "            bos_token_id=bos_token_id,\n",
        "            pad_token_id=self.tokenizer.eos_token_id,\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            max_length=max_output_length,\n",
        "            top_p=0.90,\n",
        "            num_return_sequences=num_return_sequences,\n",
        "        )\n",
        "        for sample_output in sample_outputs:\n",
        "            generated_text.append(self.tokenizer.decode(sample_output, skip_special_tokens=True))\n",
        "        return generated_text\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return AdamW(self.model.parameters(), lr=self.lr, eps=self.epsilon)\n",
        "\n",
        "    def configure_scheduler(self, num_warmup_steps, num_training_steps):\n",
        "        return get_linear_schedule_with_warmup(\n",
        "            self.configure_optimizers(), num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps\n",
        "        )\n",
        "\n",
        "    def save_model(self, output_dir):\n",
        "        # Save the model state dictionary\n",
        "        self.model.save_pretrained(output_dir)\n",
        "        # Save the tokenizer\n",
        "        self.tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "        print(\"Saving model to %s\" % output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xc3h3xlTr2gB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class GPT2Dataset(Dataset):\n",
        "    def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=768):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "\n",
        "        for txt in txt_list:\n",
        "            encodings_dict = tokenizer(\n",
        "                \"<|startoftext|>\" + txt + \"<|endoftext|>\", truncation=True, max_length=max_length, padding=\"max_length\"\n",
        "            )\n",
        "\n",
        "            self.input_ids.append(torch.tensor(encodings_dict[\"input_ids\"]))\n",
        "            self.attn_masks.append(torch.tensor(encodings_dict[\"attention_mask\"]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.attn_masks[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sqDGKjysXf7"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def get_data():\n",
        "    dataset = load_dataset(\"izumi-lab/open-text-books\")\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def process_data(rawDataset):\n",
        "    dataset = pd.DataFrame(rawDataset)\n",
        "\n",
        "    array_of_books = []\n",
        "    for a in dataset[\"train\"]:\n",
        "        array_of_books.append(a[\"text\"])\n",
        "\n",
        "    df = pd.DataFrame(array_of_books, columns=[\"text\"])\n",
        "\n",
        "    df.dropna(inplace=True)  # remove NA values\n",
        "    return df\n",
        "\n",
        "\n",
        "data = process_data(get_data())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import wandb\n",
        "import random\n",
        "import datetime\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, random_split, RandomSampler, SequentialSampler\n",
        "\n",
        "\n",
        "# loading\n",
        "# config = OmegaConf.load(\"project_name/config.yaml\")\n",
        "\n",
        "parameter = {\n",
        "    \"epochs\": config[\"hyperparameters\"][\"epochs\"],\n",
        "    \"learning_rate\": config[\"hyperparameters\"][\"learning_rate\"],\n",
        "    \"warmup_steps\": config[\"hyperparameters\"][\"warmup_steps\"],\n",
        "    \"epsilon\": config[\"hyperparameters\"][\"epsilon\"],\n",
        "    \"batch_size\": config[\"hyperparameters\"][\"batch_size\"],\n",
        "    # this produces sample output every 100 steps\n",
        "    \"sample_every\": config[\"hyperparameters\"][\"sample_every\"],\n",
        "    \"seed_val\" :config[\"hyperparameters\"][\"seed_val\" ]\n",
        "}\n",
        "\n",
        "\n",
        "def format_time(elapsed):\n",
        "    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n",
        "\n",
        "\n",
        "def dataloader(tokenizer, batch_size):\n",
        "    df = data.sample(n=3000, random_state=parameter[\"seed_val\"])\n",
        "    dataset = GPT2Dataset(df['text'], tokenizer, max_length=768)\n",
        "    print(len(dataset))\n",
        "\n",
        "    # Split into training and validation sets\n",
        "    train_size = int(0.9 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    print(\"{:>5,} training samples\".format(train_size))\n",
        "    print(\"{:>5,} validation samples\".format(val_size))\n",
        "\n",
        "    # Create the DataLoaders for our training and validation datasets.\n",
        "    # We'll take training samples in random order.\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,  # The training samples.\n",
        "        sampler=RandomSampler(train_dataset),  # Select batches randomly\n",
        "        batch_size=batch_size,  # Trains with this batch size.\n",
        "    )\n",
        "\n",
        "    # For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "    validation_dataloader = DataLoader(\n",
        "        val_dataset,  # The validation samples.\n",
        "        sampler=SequentialSampler(val_dataset),  # Pull out batches sequentially.\n",
        "        batch_size=batch_size,  # Evaluate with this batch size.\n",
        "    )\n",
        "    return train_dataloader, validation_dataloader\n"
      ],
      "metadata": {
        "id": "e9iSyAmz2hr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b106800b14514a63be6440c85c449009",
            "2089d86aa45e4dc88f51f13b773ca504",
            "dd3316730ecb437c8898fa61c2a31e92",
            "9cfd9fc859aa4413872a3ab1973a1e25",
            "77c6bfa1566346f3ad754baf8867fd5e",
            "b4e13c5aa8ab4ff19df4a75559a3df2b",
            "96701f8c2c7e45bcbdab1ce0b19128bf",
            "fbcd197718804e79aa384de3860fcda1"
          ]
        },
        "id": "N2rhC-AIr2gB",
        "outputId": "e20dd634-52d0-48ad-a8b2-33cdf82662bb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240117_144546-bjvevihc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tinghui/mlops_g30/runs/bjvevihc' target=\"_blank\">fearless-sun-27</a></strong> to <a href='https://wandb.ai/tinghui/mlops_g30' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/tinghui/mlops_g30' target=\"_blank\">https://wandb.ai/tinghui/mlops_g30</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/tinghui/mlops_g30/runs/bjvevihc' target=\"_blank\">https://wandb.ai/tinghui/mlops_g30/runs/bjvevihc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000\n",
            "2,700 training samples\n",
            "  300 validation samples\n",
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch   200  of  1,350. Loss: 3.370478868484497.   Elapsed: 0:01:44.\n",
            "0:  bipartisanA said a person may have a serious problem with the issue. This could be that they are engaged in a serious problem with their relationship between the person and the person, especially if the situation arises of a child or a child.\n",
            "A person may have a significant problem with the relationship between the person and the person because of the person.\n",
            "\n",
            "A person may have a serious problem with the relationship between the person and the person, especially if the situation arises from a child or a child.\n",
            "A person may not have a significant problem with the relationship between the person and the person because of the person or the relationship between the person and the person.\n",
            "A person may have a serious problem with the relationship between the person and the person, especially if the situation arises from a child or a child.\n",
            "The person and the relationship between the person and the person may have a serious problem with the relationship between the person and the person.\n",
            "\n",
            "This may be an obstacle to a person in\n",
            "1:  bipartisanIn an essay published by the, a male physicist says that it has no matter where in the world he is or what he thinks. To prove this, he must always find the most suitable object. He must, however, find the most suitable object.\n",
            "This article is not about a piece of paper or a piece of paper; it is about a piece of paper. It is written on the paper or whatever. It is not necessarily the most suitable object. The articles have only one line or section. It is not necessarily a perfect object. There is one line or section. The articles have only one section. It is a perfect object. It is not perfect.\n",
            "A perfect object is perfect because it does not make the perfect object. However, it is the perfect object to which a perfect object is perfect, and it is the perfect object to which a perfect object is perfect. There is no perfect object in this article. There is no perfect object in this article\n",
            "2:  bipartisanThe case against the legalisation of marriage – but what about the public right to marriage?\n",
            "The courts should consider how courts treat marriage.\n",
            "What about the legalisation of marriage?\n",
            "There is a large minority of marriage cases – some of which require the court to consider the circumstances in which it should be performed. This is not a decision on a person, but on the basis of the law.\n",
            "It may be that an act of marriage is not necessary to protect the child. In such cases, the judge should decide whether it is legal and therefore appropriate.\n",
            "When the court does so, the family, including the parents, should take up all necessary legal proceedings and make their decision.\n",
            "A judge should consider whether it is proper to make a judgment to protect the child in the event it is necessary to do so. The judge also should consider whether a person should be able to enter into marriage with the family.\n",
            "A judge should consider whether a marriage is possible and whether it\n",
            "  Batch   400  of  1,350. Loss: 2.0823819637298584.   Elapsed: 0:03:31.\n",
            "0:  increasingThe extent of the data compression varies in a context that is different from the context in which we can use the compression. However, the compression can vary in many ways depending on whether the context is the same as the context in which the data is being compression. The majority of the data compression are obtained from data that is large enough to be able to be used later, but some may not be as large as the context in which the data is being compression.\n",
            "\n",
            "While data compression is most commonly used by the majority of the software engineers, it is also used by other software engineers to perform the same task or task that is associated with the program. It can be used by both software engineers and programmers to perform tasks or tasks associated with a program.\n",
            "\n",
            "1:  increasingThese types of exercises are often used in many different contexts: in the arts, in traditional forms of dance, and in other contexts, such as a ballet, a dance, or a dance-based movement. This type of exercise helps to enhance the role of body and body-focused movements in understanding human emotion, making it easier to assess the body and how to interpret the body (Figure 1.4).\n",
            "\n",
            "These exercises, however\tdo not work in all contexts; for example, a dance is a motion in which movement is not performed. If a dancer is able to move beyond these exercises, the body and body-focused movement becomes more apparent.\n",
            "\n",
            "In general, exercises with both body and body-focused movement are best suited for individuals to use in contexts in which their body and body-focused movements occur. Therefore, although they can be very useful for the student, they are not intended to be used in everyday practice.\n",
            "Figure 1.5. Use of a\n",
            "2:  increasingThis is a very rare example of a person having a heart condition. For example, a person with a heart condition, and an obese person in the general population, could have a heart condition. In such cases, the weight in the body can be significant, but it can not be a cause for immediate death.\n",
            "\n",
            "A person with a heart condition can have a major depression, or chronic feeling of depression. A person with a heart condition can have a significant difficulty falling asleep. People with a cardiovascular condition, and the type of depression are also more likely to have a severe depression. People with a depression may experience difficulty falling asleep during a week, sometimes even during the day, as a result of the stress.\n",
            "\n",
            "People with a heart condition can have a significant difficulty falling asleep. When this occurs, the body can be hard and intense. It can sometimes be a problem with breathing, but it is not a problem when the heart is hard and the body is strong. When the\n",
            "  Batch   600  of  1,350. Loss: 0.42600399255752563.   Elapsed: 0:05:18.\n",
            "0: dayThis is the type of language that can be used with a certain amount of care. It is often the only word that can be used when an application is developing, and is not the only word to be used when an application needs to learn some other thing. However, this is not the only way that programmers can express their language. Some languages can express only a single language. Most languages have different levels of semanticization. However, even when coding languages are creating their language, one can still get the point across. The fact that a language has no language level, even when coding is making progress, means that you must have a way to express all of the language features. For example, languages such as Java, Python, Java, and Python are languages with different levels of semanticization. The goal of making progress is to have a way to express all of the language features.\n",
            "\n",
            "Java is the second language to be discussed. Java is the second language to be discussed. Java is\n",
            "1: dayThe main objection to this interpretation of the story—that it is based on a plotline—was that a plotline involves three individuals: The protagonist of the story is a man called the narrator, and the narrator is a man called the antagonist, but this interpretation may have had a different meaning. The protagonist’s story is based on a plotline, not an individual plotline. The narrator is based on a plot, not an individual plotline. The narrator has to deal with problems, problems, problems, problems. In the story, each problem is represented by a plotline, but the problem is actually more the result of the story than any individual plotline.\n",
            "However, since the story is based on a plotline, no problems are apparent. The story does not assume that the character in the story has to deal with problems, problems, problems, problems. The narrator does not expect problems to be solved; he has only a set of options to choose from.\n",
            "2: day\n",
            " (. $3, $3, $3, $3, $3, $3, $3) = $3.0\n",
            "\n",
            "                      , $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $3, $\n",
            "  Batch   800  of  1,350. Loss: 0.5499376058578491.   Elapsed: 0:07:03.\n",
            "0:  Hanga = 1 + 1 + 2 = 2 + 3 + 4 + 5 + 6 = 3 + 6 + 7 = 3 + 8 + 8 + 9 = 3 + 9 = 3 + 8 + 9 + 10 = 3 + 9 + 10 = 3 + 8 + 9 + 10 + 11 = 3 + 8 + 9 + 10 + 11 = 3 + 8 + 9 + 10 + 11 + 11 = 3 + 8 + 9 + 11 = 3 + 8 + 9 + 10 + 11 + 11 = 3 + 8 + 9 + 11 + 12 = 3 + 8 + 9 + 11 + 12 = 3 + 8 + 9 + 11 + 12 = 3 + 8 + 9 + 11 = 3 + 8 + 9 + 11 + 12 = 3 + 8 + 9 + 11 = 3 + 8 + 9 + 11 = 3 + 8 + 9 + 12 = 3 + 8 + 9 + 11 = 3 + 8 + 9 + 11 = 3 + 8 + 9 + 11 = 3\n",
            "1:  HangThe following diagram shows how a computer system’s internal system functions, and what it would cost to create the system if it weren’t in operation.\n",
            "2:  HangIt is not possible to tell whether or not a particular piece of paper has a particular point of interest, such as the identity of a piece of paper to represent an idea or not. For the purpose of this chapter, we will take the name of a piece of paper and make a statement to it. The name of a piece of paper may be: “The identity of an idea” (The identity of a piece of paper).\n",
            "This statement is a simple one, but it can be helpful for any student. The name of a piece of paper should be: “The identity of an idea” (The identity of an idea).\n",
            "Write your message as a simple as you can, and if you are reading this, you should probably use it. Use the form of “The identity of an idea” to write your message.\n",
            "  Batch 1,000  of  1,350. Loss: 2.2052152156829834.   Elapsed: 0:08:49.\n",
            "0:  foodsWe can now make a list of how many hours of practice we must complete during a day, and how long we must complete it each day. This might seem simple enough, but it’s important to remember that during the day, we will need to do things like write notes, make notes, and review notes on a computer and computer.\n",
            "\n",
            "Learning How to Read\n",
            "If we are starting from scratch and doing more than just reviewing, we need to make sure the computer and our hands are ready to move. How long it must take us to hearts is an important piece of the picture.\n",
            "Sometimes it comes as a surprise to see someone like A, C, D, E, H, J, L, L, M, N, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R,\n",
            "1:  foodsThe effect of age on cognition and memory is important for understanding how the brain has function-modulating genes. Although the brain is not immune to age-related changes in the brain, there is a risk of developing Alzheimer’s disease. The brain is also vulnerable to changes in the brain that may contribute to cognitive decline as well as cognitive decline in older adults who are less educated. Because of age-related risk factors, it is important to understand the impact of these changes. This review will focus on the main types of learning changes affecting cognition, memory, and judgment. Learning change patterns are associated with age, education, and IQ scores; they are also associated with the effect of age on cognition and memory.\n",
            "Learning change patterns are associated with the age-related effect of age on cognition and memory. The effect of age on cognition is important for understanding how it relates to cognitive decline. It is important to remember that all learning change patterns are associated with some of the primary risk factors\n",
            "2:  foodsSolving an equation and solving a simple regression equation with the regression equation can be a great way to find the regression equations of your equations. You can find the regression equation of a linear equation by comparing the square root solution, and the solution.\n",
            "\n",
            "The following information will help us better understand the regression equation of a linear equation.\n",
            "  Batch 1,200  of  1,350. Loss: 0.7481894493103027.   Elapsed: 0:10:35.\n",
            "0:  trail”\n",
            "In English, a pronoun refers to an event or part of something happening. This is a particularly common subject for verbs. The noun “fools” means the object of a verb. This is a verb with an “finale” suffix, meaning “fools are here,” and the verb in “finale” is an expression. In German, for example, the verb “we” is also a verb, meaning “we” is always used as an expression. In other words, in German, “finale” means “finale” but it may also mean “finale” in German.\n",
            "1:  trailThe most common error, however, is that the correct answer is not “good.'” Instead, the correct answer is “good” or “no”. The correct answer is “good” or “no”.\n",
            "2:  trailTreatment can be given by placing the finger tip down on the side of the neck of the head. It is not necessary to touch the head to keep the ear out of the ear. A small finger will be placed beneath the ear, and the thumb and index finger is close to the ear. All fingers in the ear should be placed at the base of the head of the head and the thumb and index finger on the upper ear should be located at the left side of the ear.\n",
            "Treatment can be given by placing the finger tip down on the head, or on the hand or palm of the fingers. It is not necessary to touch the ear to keep the ear out of the ear. A small finger tip on the head is placed beneath the ear, and the thumb and index finger is close to the ear. All fingers in the ear should be placed at the top of the head. They should also be placed at the center of the ear.\n",
            "Treatment should not be\n",
            "\n",
            "  Average training loss: 1.57\n",
            "  Training epoch took: 0:11:55\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 1.29\n",
            "  Validation took: 0:00:25\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "  Batch   200  of  1,350. Loss: 2.9180798530578613.   Elapsed: 0:01:44.\n",
            "0: intendSolving an Equational Problem\n",
            "Solution\n",
            "In some situations, you may have to solve the problem of two variables. An Equational Problem is any situation where one of these is an Equational Problem.\n",
            "For example, suppose we want to solve this problem using both variables:\n",
            "If you have two equations. We can solve them using the equation (0) = (t2.t2 + t0) = t2.t0 + t0.\n",
            "1: intendThe following is the definition of the term “social order” in the United States: “the concept which sets aside the individual” and is in common use in various countries. (See  for a more detailed explanation.)”\n",
            "The term “social order” is used in various contexts in various languages. However, social order is not considered to be a “common set of values” or “religion” or “common order”. “The term social order is used in the most commonly used context.”\n",
            "The concept which sets aside the individual is in common use in various countries. But the concept which sets aside the individual is the notion which sets aside the individual.”\n",
            "Social order is the set of values, which define individuals. “Social order” is used in various contexts in different languages. However, the concept which sets aside the individual is the concept which sets aside the individual.\n",
            "2: intendThis is a significant and important development. In addition to creating open-source projects and teaching students to use open-source, open-source resources in a timely fashion, it is vital to build community around shared knowledge and open-source applications. At the same time, it is imperative that we have a positive and collaborative relationship with students, faculty, and staff. Acknowledge that the work you are creating is ultimately the product of open-source, open-source resource applications.\n",
            "At the end of this chapter, you will learn:\n",
            "Open- Source Community\n",
            "Open-source community is a community where all students, faculty, and staff collaborate and build upon the work they are developing with open-source software. This is where open-source software and open-source software programs come together to make a tangible difference in the lives of everyone involved.\n",
            "Open-source community strives to promote open-source in its own right.\n",
            "The goal is to help students and staff apply knowledge\n",
            "  Batch   400  of  1,350. Loss: 0.4520321488380432.   Elapsed: 0:03:30.\n",
            "0:  surroundA key component to a single page is the header. When you copy a single paragraph into an email, you will want to make sure that the header is intact. For instance, if you need to edit the first part of a paragraph, you can use the copy tag instead of the copy label. A typical example is to add the first line of a paragraph. To create the header you can use the copy tag.\n",
            "\n",
            "For your email, you may wish to ensure that you have enough room for all the details, including the content, of the message. For example, if you want to create a separate email, you may want to have at least one of the details in the email: the email address you want to create, the email page that you want to send out, the email topic you want to discuss, the content you want to discuss, the subject your email says you want to share. Because you are using email, you must be sure you\n",
            "1:  surroundThe second half of Chapter 1 illustrates the use of algebra-making to produce results in the future. For instance, imagine that you have studied geometry, physics, or chemistry, and you are sharing your ideas for the next book. How would you share your ideas with your fellow students or to the university library in a way that will generate more than a few more questions and/or answers? What would you do to the students who were involved in that project and to those who had to go to the library and get feedback from them? This would be a very important study in its own right.\n",
            "\n",
            "The first thing you should know about algebra is that you must have familiarity with algebra, as well as familiarity with algebra students, and that you can create algebra in the most effective and fun way possible. It is important for you to understand how to apply algebra to different situations, including the classroom and the student-made videos. In order to make use of algebra, you should have a reasonable\n",
            "2:  surroundTo illustrate, consider a small example of the concept of the inverse square: A (small) rectangular box. The two sides of the box (the smallest box and the smallest one) are called the box sides. In, the box sides are the rectangular box. The size of the box is called the squares.\n",
            "  Batch   600  of  1,350. Loss: 0.9723880887031555.   Elapsed: 0:05:16.\n",
            "0:  reflexThe two primary points in, the first points, and the second points are the values of the values of the values of variables.\n",
            "The second points are the values of variables (e.g., the values of the variables of a particular point).\n",
            "\n",
            "In this example, there are three variables:\n",
            "\n",
            "We now know that we have three variables, in other words, the three variables:\n",
            "1:  reflexThe standard deviation of the mean (i.e., the mean) is obtained by computing the integral ρ = f ( f−1, r = r), and the interval ρ = f ( f   r, r = r +1, r = r +1, r = ρ ∃x, ∃y ). Then f ( f−1, r = r) = f ( f, r ) = (\t\n",
            "\n",
            "An iterative operation\n",
            "In this application, we will define and create iterative functions that can be iterated through a list of vertices or arrays of arrays that are defined in.\n",
            "\n",
            "It is useful to check that if any of the ordered pairs are of the right size, the iterative function returns the sum of the ordered pairs of the right size.\n",
            "An iterative operation\n",
            "We now have two iterative functions to perform, and we can define them to iterate through the array or array of arrays. This\n",
            "2:  reflexWe will assume that the first of the arguments in (x, y) (n, x, y) is the same as the last argument. Thus, we must multiply this expression with its corresponding argument in (x, y) (n, x, y) and apply the second rule. Note that we must use the expression, n, to convert to a formula of the first variable, n, to the second variable. The last variable may have any of the variables n as argument variable, but not any of its arguments.\n",
            "\n",
            "We begin with the first variable (1), and use it to convert it to a formula of the first variable (2), and apply the second rule.\n",
            "We add (x, y) to n the last variable.\n",
            "\n",
            "We assign (y, n) to x in the last variable and apply the second rule.\n",
            "We convert (y, n) to a formula of n (2), but do not change the value of\n",
            "  Batch   800  of  1,350. Loss: 1.7396669387817383.   Elapsed: 0:07:03.\n",
            "0:  displayIt is necessary to be sure that there are two or more of the following conditions. First, we must know that we will find that if two particles form the same pair of atoms, and if two particles form different combinations of atoms, then the atoms will have the same weight. If the two particles form the same pair of atoms, then the atoms will have the same weight. If they are not the same weight, the atoms will have the same weight.\n",
            "\n",
            "Theorem: To be sure that two particles have the same weight, the quantities (i.e., a and b, c, d, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e,\n",
            "1:  displayThe term “black hole” was coined by a scientist to describe the theory that black hole orbits around us. In the scientific terminology, an outer tube, or tube, is an outer cavity that has closed down, forming the solid phase of the black hole. The outer tube is much like a wormhole, but in the image above is a wormhole that has opened in a small plane of motion. The shape and type of the wormhole depends on the type of the hole.\n",
            "Figure 5.13 The wormhole is shown here to the right. The wormhole has been an outer tube since its construction in the 1970s. (credit: David Hershey, Flickr)\n",
            "Black hole theory was defined by black hole theory as that “the concept of the hole” means that one’s own energy is the electric energy, or energy of the surrounding gas, whereas black hole theory was based on the theory of how this energy moves. In an analogy to\n",
            "2:  displayIf you look at the table in Figure 8.4, you will see two groups of values. As with Figure 8.5, they are all positive.\n",
            "  Batch 1,000  of  1,350. Loss: 0.5917109847068787.   Elapsed: 0:08:49.\n",
            "0:  pastorThis is what happens when an object is stolen. If the object’s security is compromised, the user loses access to the item and is not able to access the item’s key signature. If the item is stolen, the user may have to choose between remaining or adding a second key. The key signature becomes less important than the item’s security, so the third key is simply a key. The third key is used to the security key, but not the owner’s key.\n",
            "In an attempt to protect the key, the third key is used when a key is stolen. The third key is used in the transaction between the owner’s key and the third key. The third key is used to the security key, so the third key is used. The third key is used to the security key, so the third key is used. The third key is used to the security key, so the third key is used. The third key is used\n",
            "1:  pastor“A person’s relationship with a particular person is a mental relationship with a person. When I am with a person, I understand that I am in an intimate relationship.”\n",
            "2:  pastor“We get this.” “We get that.” “We get that.”\n",
            "The second element is the minus sign. The minus sign (or minus sign in the third) means that we multiply the minus sign by the minus sign and the minus sign are the same, except that they give us a minus sign. The minus sign is a sign that is either added or subtracted from the minus sign. A minus sign can be subtracted from a minus sign and it has negative sign values. When a minus sign is subtracted into a minus sign and subtracted from the minus sign, it has a minus sign.\n",
            "  Batch 1,200  of  1,350. Loss: 0.4943425953388214.   Elapsed: 0:10:36.\n",
            "0:  illicitIt is clear that this is not a simple phrase. In fact, for this to succeed, we must consider only one word, and that’s “work.” It is only at the moment that we realize we need to use a more productive term. I’ll explain that in more detail, and I believe it helps. For example, you can do this by looking at the word “work” and noting that it’s “working,” so you don’t want to use a better term.\n",
            "Let’s look at some examples:\n",
            "“A little girl called her best girlfriend,” a name you can identify by using the right pronoun. It’s a simple example, because she was a girlfriend for a while at school, and her girlfriend wasn’t even real. You can use the word “work” as a way of expressing what she was feeling or wanting to\n",
            "1:  illicitBy the end of this section, you will be able to:\n",
            "Acknowledge that some of your research priorities are more aligned to your specific strengths and your interests\n",
            "Use information from this textbook to answer your questions about the content of each section and whether they are appropriate to your goals\n",
            "Use this textbook as a guide for any new or fresh ideas you find that may help you evaluate and expand your understanding of how to organize your work\n",
            "Use this textbook as a guide for any new or improved ideas you discover. For example, when making your decision to add a new chapter, you can determine whether your understanding of the material is a good fit with how you want to work with colleagues.\n",
            "As with all good research, it may be a good idea to check the answers to your research priorities before making changes.\n",
            "\n",
            "Using the text, you can be a great help to your colleagues as they try to solve real-world problems for you. If you find yourself needing to revise your answer\n",
            "2:  illicitNote that the number of terms on the right of a variable will vary greatly throughout the user interface, and the user interface may change frequently in order to provide a more user friendly interface. Some examples are shown below.\n",
            "\n",
            "  Average training loss: 1.23\n",
            "  Training epoch took: 0:11:56\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 1.34\n",
            "  Validation took: 0:00:25\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:24:55 (h:mm:ss)\n",
            "Saving model to models/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b106800b14514a63be6440c85c449009"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Batch Loss</td><td>█▅▁▁▅▂▇▁▂▄▁▁</td></tr><tr><td>Training Loss</td><td>█▁</td></tr><tr><td>Valid. Loss</td><td>▁█</td></tr><tr><td>epoch</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Batch Loss</td><td>0.49434</td></tr><tr><td>Training Loss</td><td>1.23138</td></tr><tr><td>Valid. Loss</td><td>1.33792</td></tr><tr><td>epoch</td><td>2</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fearless-sun-27</strong> at: <a href='https://wandb.ai/tinghui/mlops_g30/runs/bjvevihc' target=\"_blank\">https://wandb.ai/tinghui/mlops_g30/runs/bjvevihc</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240117_144546-bjvevihc/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def train():\n",
        "\n",
        "    random.seed(parameter[\"seed_val\"])\n",
        "    torch.manual_seed(parameter[\"seed_val\"])\n",
        "    torch.cuda.manual_seed_all(parameter[\"seed_val\"])\n",
        "    # Saving loss\n",
        "    training_stats = []\n",
        "\n",
        "    total_t0 = time.time()\n",
        "    wandb.init(project=\"mlops_g30\",config= parameter)\n",
        "    # A GPT model with arguments \"lr\" of learning rate & \"eps\" of epsilon\n",
        "    model = Model(lr=parameter[\"learning_rate\"], eps=parameter[\"epsilon\"])\n",
        "    tokenizer = model.tokenizer\n",
        "\n",
        "    # Tell pytorch to run this model on the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "    model = model.to(device)\n",
        "    #self.model.cuda()\n",
        "\n",
        "    # Load the data\n",
        "    train_dataloader, valid_dataloader = dataloader(tokenizer, parameter[\"batch_size\"])\n",
        "\n",
        "\n",
        "    # this step is necessary because I've added some tokens (bos_token, etc) to the embeddings\n",
        "    # otherwise the tokenizer and model tensors won't match up\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    # Total number of training steps is [number of batches] x [number of epochs].\n",
        "    # (Note that this is not the same as the number of training samples).\n",
        "    total_steps = len(train_dataloader) * parameter[\"epochs\"]\n",
        "    # Create the optimizer(AdamW)\n",
        "    optimizer = model.configure_optimizers()\n",
        "    # Create the learning rate scheduler.\n",
        "    # This changes the learning rate as the training loop progresses\n",
        "    scheduler = model.configure_scheduler(num_warmup_steps=parameter[\"warmup_steps\"], num_training_steps=total_steps)\n",
        "\n",
        "    for epoch_i in range(0, parameter[\"epochs\"]):\n",
        "        # ========================================\n",
        "        #               Training\n",
        "        # ========================================\n",
        "        print(\"\")\n",
        "        print(\"======== Epoch {:} / {:} ========\".format(epoch_i + 1, parameter[\"epochs\"]))\n",
        "        print(\"Training...\")\n",
        "\n",
        "        # Save the starting training time for epoch\n",
        "        t0 = time.time()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        # start training mode\n",
        "        model.train()\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            loss = batch_train(model, device, batch)\n",
        "            batch_loss = loss.item()\n",
        "            total_train_loss += batch_loss\n",
        "\n",
        "            # Get sample every x batches.\n",
        "            if step % parameter[\"sample_every\"] == 0 and not step == 0:\n",
        "                elapsed = format_time(time.time() - t0)\n",
        "                print(\n",
        "                    \"  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.\".format(\n",
        "                        step, len(train_dataloader), batch_loss, elapsed\n",
        "                    )\n",
        "                )\n",
        "                wandb.log({\"Batch Loss\": batch_loss})\n",
        "                sample(model)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        # Calculate the average loss over all of the batches.\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "        # Measure how long this epoch took.\n",
        "        training_time = format_time(time.time() - t0)\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "        print(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "        # ========================================\n",
        "        #               Validation\n",
        "        # ========================================\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Running Validation...\")\n",
        "        avg_val_loss, validation_time = valid(model, device, valid_dataloader)\n",
        "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "        print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "        # Record all statistics from this epoch.\n",
        "        training_stats.append(\n",
        "            {\n",
        "                \"epoch\": epoch_i + 1,\n",
        "                \"Training Loss\": avg_train_loss,\n",
        "                \"Valid. Loss\": avg_val_loss,\n",
        "                \"Training Time\": training_time,\n",
        "                \"Validation Time\": validation_time,\n",
        "            }\n",
        "        )\n",
        "        # log metrics to wandb\n",
        "        wandb.log({\"epoch\": epoch_i + 1,\n",
        "                    \"Training Loss\": avg_train_loss,\n",
        "                    \"Valid. Loss\": avg_val_loss})\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Training complete!\")\n",
        "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time() - total_t0)))\n",
        "\n",
        "    # ========================================\n",
        "    #               Save\n",
        "    # ========================================\n",
        "    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "    output_dir = \"models/\"\n",
        "\n",
        "    # Create output directory if needed\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "    # They can then be reloaded using `from_pretrained()`\n",
        "    model.save_model(output_dir)\n",
        "    wandb.finish()\n",
        "\n",
        "def batch_train(model, device, batch):\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_labels = batch[0].to(device)\n",
        "    b_masks = batch[1].to(device)\n",
        "\n",
        "    model.zero_grad()\n",
        "\n",
        "    outputs = model(input_ids=b_input_ids, labels=b_labels, attention_mask=b_masks)\n",
        "    loss = outputs[0]\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def sample(model):\n",
        "    model.eval()\n",
        "\n",
        "    sample_outputs = model.generate(bos_token_id=random.randint(1, 30000), num_return_sequences=3)\n",
        "    for i, sample_output in enumerate(sample_outputs):\n",
        "        print(\"{}: {}\".format(i, sample_output))\n",
        "\n",
        "    model.train()\n",
        "\n",
        "\n",
        "def valid(model, device, valid_dataloader):\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_loss = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in valid_dataloader:\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[0].to(device)\n",
        "        b_masks = batch[1].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=b_input_ids, attention_mask=b_masks, labels=b_labels)\n",
        "\n",
        "            loss = outputs[0]\n",
        "\n",
        "        batch_loss = loss.item()\n",
        "        total_eval_loss += batch_loss\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(valid_dataloader)\n",
        "\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    return avg_val_loss, validation_time\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "output_dir = \"models/\"\n",
        "\n",
        "# Encode a text inputs\n",
        "text = \"What is the fastest car in the\"\n",
        "\n",
        "model = Model(model_version=output_dir)\n",
        "\n",
        "# Set the model in evaluation mode to deactivate the DropOut modules\n",
        "model.eval()\n",
        "\n",
        "\n",
        "\n",
        "def make_prediction(model, input_prompt, max_output_length):\n",
        "    \"\"\"\n",
        "\n",
        "    Args:\n",
        "        model: Model used for making predictions\n",
        "        input_prompt (str): input prompt for the model, that will be used to generate text\n",
        "        max_output_length (int): The maximum length for the\n",
        "\n",
        "    Returns:\n",
        "        prediction (str): input + text generated by the model in a string\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    indexed_tokens = model.tokenizer.encode(input_prompt)\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    prediction = model.generate(inputs=tokens_tensor, max_output_length=max_output_length, num_return_sequences=1)\n",
        "\n",
        "    return prediction[0]\n",
        "\n",
        "# Print the predicted word\n",
        "print(make_prediction(model,text,300))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "9nzlZGql0H4h",
        "outputId": "52313f5f-2c50-475c-c165-f1f28a891f42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-c46fef14510c>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Print the predicted word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-c46fef14510c>\u001b[0m in \u001b[0;36mmake_prediction\u001b[0;34m(model, input_prompt, max_output_length)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mindexed_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mtokens_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexed_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokens_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_output_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_output_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-03d8f240bc6d>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, bos_token_id, max_output_length, num_return_sequences)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Use the generate method from the GPTmodel model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mgenerated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         sample_outputs = self.model.generate(\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mbos_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbos_token_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m             \u001b[0;31m# 13. run sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1719\u001b[0;31m             return self.sample(\n\u001b[0m\u001b[1;32m   1720\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2800\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2801\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2802\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2803\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1094\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp -r models /content/drive/MyDrive/dtu_mlops"
      ],
      "metadata": {
        "id": "03ADc2552L91"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b106800b14514a63be6440c85c449009": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2089d86aa45e4dc88f51f13b773ca504",
              "IPY_MODEL_dd3316730ecb437c8898fa61c2a31e92"
            ],
            "layout": "IPY_MODEL_9cfd9fc859aa4413872a3ab1973a1e25"
          }
        },
        "2089d86aa45e4dc88f51f13b773ca504": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77c6bfa1566346f3ad754baf8867fd5e",
            "placeholder": "​",
            "style": "IPY_MODEL_b4e13c5aa8ab4ff19df4a75559a3df2b",
            "value": "0.036 MB of 0.036 MB uploaded\r"
          }
        },
        "dd3316730ecb437c8898fa61c2a31e92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96701f8c2c7e45bcbdab1ce0b19128bf",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fbcd197718804e79aa384de3860fcda1",
            "value": 1
          }
        },
        "9cfd9fc859aa4413872a3ab1973a1e25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77c6bfa1566346f3ad754baf8867fd5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4e13c5aa8ab4ff19df4a75559a3df2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "96701f8c2c7e45bcbdab1ce0b19128bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbcd197718804e79aa384de3860fcda1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}